{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Head:\n",
      "  tourney_id tourney_name surface  draw_size tourney_level  tourney_date  \\\n",
      "0   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "1   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "2   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "3   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "4   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "\n",
      "   match_num  winner_id     winner_name winner_hand  ...  l_1stIn l_1stWon  \\\n",
      "0          1     104053    Andy Roddick           R  ...     34.0     29.0   \n",
      "1         30     103285  Radek Stepanek           R  ...     27.0     14.0   \n",
      "2         29     104053    Andy Roddick           R  ...     43.0     34.0   \n",
      "3         28     103285  Radek Stepanek           R  ...     40.0     25.0   \n",
      "4         27     104792    Gael Monfils           R  ...     50.0     38.0   \n",
      "\n",
      "   l_2ndWon  l_SvGms l_bpSaved l_bpFaced  winner_rank winner_rank_points  \\\n",
      "0      11.0     10.0       3.0       5.0          7.0             4410.0   \n",
      "1       7.0      7.0       3.0       7.0         12.0             2625.0   \n",
      "2      21.0     13.0      10.0      12.0          7.0             4410.0   \n",
      "3      11.0     10.0       6.0      10.0         12.0             2625.0   \n",
      "4      17.0     14.0       3.0       6.0         13.0             2610.0   \n",
      "\n",
      "   loser_rank loser_rank_points  \n",
      "0        77.0             598.0  \n",
      "1        13.0            2610.0  \n",
      "2        20.0            1655.0  \n",
      "3       105.0             521.0  \n",
      "4        44.0             935.0  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "\n",
      "Columns in Dataset:\n",
      "Index(['tourney_id', 'tourney_name', 'surface', 'draw_size', 'tourney_level',\n",
      "       'tourney_date', 'match_num', 'winner_id', 'winner_name', 'winner_hand',\n",
      "       'winner_ht', 'winner_ioc', 'winner_age', 'loser_id', 'loser_name',\n",
      "       'loser_hand', 'loser_ht', 'loser_ioc', 'loser_age', 'score', 'best_of',\n",
      "       'round', 'minutes', 'w_ace', 'w_df', 'w_svpt', 'w_1stIn', 'w_1stWon',\n",
      "       'w_2ndWon', 'w_SvGms', 'w_bpSaved', 'w_bpFaced', 'l_ace', 'l_df',\n",
      "       'l_svpt', 'l_1stIn', 'l_1stWon', 'l_2ndWon', 'l_SvGms', 'l_bpSaved',\n",
      "       'l_bpFaced', 'winner_rank', 'winner_rank_points', 'loser_rank',\n",
      "       'loser_rank_points'],\n",
      "      dtype='object')\n",
      "\n",
      "X_train shape: (73010, 13)\n",
      "X_train dtype: float64\n",
      "X_test shape:  (12018, 13)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"atp_matches_2010_2024_missing_handled.csv\")\n",
    "\n",
    "print(\"Dataset Head:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "print(\"\\nColumns in Dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Encode categorical variable: surface\n",
    "modelLR_df = pd.get_dummies(df, columns=[\"surface\"], drop_first=True)\n",
    "\n",
    "# Convert bool to int for one-hot encoded columns\n",
    "for col in [\"surface_Grass\", \"surface_Hard\"]:\n",
    "    modelLR_df[col] = modelLR_df[col].astype(int)\n",
    "\n",
    "modelLR_df[\"rank_diff\"] = modelLR_df[\"winner_rank\"] - modelLR_df[\"loser_rank\"]\n",
    "modelLR_df[\"ace_diff\"] = modelLR_df[\"w_ace\"] - modelLR_df[\"l_ace\"]\n",
    "modelLR_df[\"df_diff\"] = modelLR_df[\"w_df\"] - modelLR_df[\"l_df\"]\n",
    "modelLR_df[\"svpt_diff\"] = modelLR_df[\"w_svpt\"] - modelLR_df[\"l_svpt\"]\n",
    "modelLR_df[\"1stIn_diff\"] = modelLR_df[\"w_1stIn\"] - modelLR_df[\"l_1stIn\"]\n",
    "modelLR_df[\"1stWon_diff\"] = modelLR_df[\"w_1stWon\"] - modelLR_df[\"l_1stWon\"]\n",
    "modelLR_df[\"2ndWon_diff\"] = modelLR_df[\"w_2ndWon\"] - modelLR_df[\"l_2ndWon\"]\n",
    "modelLR_df[\"SvGms_diff\"] = modelLR_df[\"w_SvGms\"] - modelLR_df[\"l_SvGms\"]\n",
    "modelLR_df[\"bpSaved_diff\"] = modelLR_df[\"w_bpSaved\"] - modelLR_df[\"l_bpSaved\"]\n",
    "modelLR_df[\"bpFaced_diff\"] = modelLR_df[\"w_bpFaced\"] - modelLR_df[\"l_bpFaced\"]\n",
    "modelLR_df[\"age_diff\"] = modelLR_df[\"winner_age\"] - modelLR_df[\"loser_age\"]\n",
    "\n",
    "\n",
    "feature_cols = [\n",
    "    \"rank_diff\", \"ace_diff\", \"df_diff\", \"svpt_diff\", \"1stIn_diff\", \"1stWon_diff\",\n",
    "    \"2ndWon_diff\", \"SvGms_diff\", \"bpSaved_diff\", \"bpFaced_diff\", \"age_diff\",\n",
    "    \"surface_Grass\", \"surface_Hard\"  # Assuming Clay is the reference\n",
    "    ]\n",
    "\n",
    "# Target: 1 if winner is Player A (recorded winner), 0 if loser wins (flip for symmetry later)\n",
    "modelLR_df[\"target\"] = 1\n",
    "\n",
    "# Create a symmetric dataset by flipping winner/loser\n",
    "df_flipped = modelLR_df.copy()\n",
    "\n",
    "df_flipped[\"rank_diff\"] = -df_flipped[\"rank_diff\"]\n",
    "df_flipped[\"ace_diff\"] = -df_flipped[\"ace_diff\"]\n",
    "df_flipped[\"df_diff\"] = -df_flipped[\"df_diff\"]\n",
    "df_flipped[\"svpt_diff\"] = -df_flipped[\"svpt_diff\"]\n",
    "df_flipped[\"1stIn_diff\"] = -df_flipped[\"1stIn_diff\"]\n",
    "df_flipped[\"1stWon_diff\"] = -df_flipped[\"1stWon_diff\"]\n",
    "df_flipped[\"2ndWon_diff\"] = -df_flipped[\"2ndWon_diff\"]\n",
    "df_flipped[\"SvGms_diff\"] = -df_flipped[\"SvGms_diff\"]\n",
    "df_flipped[\"bpSaved_diff\"] = -df_flipped[\"bpSaved_diff\"]\n",
    "df_flipped[\"bpFaced_diff\"] = -df_flipped[\"bpFaced_diff\"]\n",
    "df_flipped[\"age_diff\"] = -df_flipped[\"age_diff\"]\n",
    "df_flipped[\"target\"] = 0\n",
    "\n",
    "# Combine original and flipped data\n",
    "df_symmetric = pd.concat([modelLR_df, df_flipped], ignore_index=True)\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "df_symmetric[\"tourney_date\"] = pd.to_datetime(df_symmetric[\"tourney_date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df_symmetric = df_symmetric.dropna(subset=[\"tourney_date\"])\n",
    "df_symmetric[\"year\"] = df_symmetric[\"tourney_date\"].dt.year\n",
    "train_data = df_symmetric[df_symmetric[\"year\"] <= 2022]\n",
    "test_data = df_symmetric[df_symmetric[\"year\"] > 2022]\n",
    "\n",
    "X_train = train_data[feature_cols].values\n",
    "y_train = train_data[\"target\"].values\n",
    "X_test = test_data[feature_cols].values\n",
    "y_test = test_data[\"target\"].values\n",
    "\n",
    "# Debug: Check array shape and type\n",
    "print(\"\\nX_train shape:\", X_train.shape)\n",
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "\n",
    "\n",
    "# Step 3: Normalize features\n",
    "X_train_mean = np.nanmean(X_train, axis=0)\n",
    "X_train_std = np.nanstd(X_train, axis=0)\n",
    "X_train_std[X_train_std == 0] = 1e-10  # Avoid division by zero with small epsilon\n",
    "X_train = np.where(np.isnan(X_train), 0, X_train)\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_test = np.where(np.isnan(X_test), 0, X_test)\n",
    "X_test = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "# Add intercept term\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the BLR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6906\n",
      "Iteration 100, Loss: 0.6109\n",
      "Iteration 200, Loss: 0.6065\n",
      "Iteration 300, Loss: 0.6062\n",
      "Iteration 400, Loss: 0.6062\n",
      "Iteration 500, Loss: 0.6062\n",
      "Iteration 600, Loss: 0.6062\n",
      "Iteration 700, Loss: 0.6062\n",
      "Iteration 800, Loss: 0.6062\n",
      "Iteration 900, Loss: 0.6062\n",
      "Iteration 1000, Loss: 0.6062\n",
      "Iteration 1100, Loss: 0.6062\n",
      "Iteration 1200, Loss: 0.6062\n",
      "Iteration 1300, Loss: 0.6062\n",
      "Iteration 1400, Loss: 0.6062\n",
      "Iteration 1500, Loss: 0.6062\n",
      "Iteration 1600, Loss: 0.6062\n",
      "Iteration 1700, Loss: 0.6062\n",
      "Iteration 1800, Loss: 0.6062\n",
      "Iteration 1900, Loss: 0.6062\n",
      "Iteration 2000, Loss: 0.6062\n",
      "Iteration 2100, Loss: 0.6062\n",
      "Iteration 2200, Loss: 0.6062\n",
      "Iteration 2300, Loss: 0.6062\n",
      "Iteration 2400, Loss: 0.6062\n",
      "Iteration 2500, Loss: 0.6062\n",
      "Iteration 2600, Loss: 0.6062\n",
      "Iteration 2700, Loss: 0.6062\n",
      "Iteration 2800, Loss: 0.6062\n",
      "Iteration 2900, Loss: 0.6062\n",
      "Iteration 3000, Loss: 0.6062\n",
      "Iteration 3100, Loss: 0.6062\n",
      "Iteration 3200, Loss: 0.6062\n",
      "Iteration 3300, Loss: 0.6062\n",
      "Iteration 3400, Loss: 0.6062\n",
      "Iteration 3500, Loss: 0.6062\n",
      "Iteration 3600, Loss: 0.6062\n",
      "Iteration 3700, Loss: 0.6062\n",
      "Iteration 3800, Loss: 0.6062\n",
      "Iteration 3900, Loss: 0.6062\n",
      "Iteration 4000, Loss: 0.6062\n",
      "Iteration 4100, Loss: 0.6062\n",
      "Iteration 4200, Loss: 0.6062\n",
      "Iteration 4300, Loss: 0.6062\n",
      "Iteration 4400, Loss: 0.6062\n",
      "Iteration 4500, Loss: 0.6062\n",
      "Iteration 4600, Loss: 0.6062\n",
      "Iteration 4700, Loss: 0.6062\n",
      "Iteration 4800, Loss: 0.6062\n",
      "Iteration 4900, Loss: 0.6062\n",
      "Estimated coefficients (Bayesian LR): [ 2.54707774e-16 -6.26407464e-02  8.72296767e-02 -6.96259897e-02\n",
      " -6.42910369e-02 -1.05908594e-02  1.80838978e-01  1.09901641e-01\n",
      "  9.83732474e-02 -7.77335495e-02 -1.99972641e-01  1.25575189e-03\n",
      " -2.07061540e-19 -1.40630019e-18]\n",
      "\n",
      "Training Accuracy (Bayesian LR): 87.24%\n",
      "Test Accuracy (Bayesian LR): 87.32%\n"
     ]
    }
   ],
   "source": [
    "# Define the sigmoid function (with clipping to prevent numerical issues)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "# Compute the negative log-posterior (loss) for Bayesian logistic regression.\n",
    "def bayes_loss(X, y, beta, tau):\n",
    "    \"\"\"\n",
    "    Computes the negative log-posterior (loss) for Bayesian Logistic Regression.\n",
    "    The loss is the negative log-likelihood plus the regularization term from the Gaussian prior.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Feature matrix with intercept column (shape: [n_samples, n_features+1])\n",
    "        y (ndarray): Binary target vector (shape: [n_samples])\n",
    "        beta (ndarray): Coefficient vector (including intercept) (shape: [n_features+1])\n",
    "        tau (float): Prior standard deviation.\n",
    "    \n",
    "    Returns:\n",
    "        loss (float): The computed loss.\n",
    "    \"\"\"\n",
    "    # Compute predictions\n",
    "    z = np.dot(X, beta)\n",
    "    y_pred = sigmoid(z)\n",
    "    \n",
    "    # Negative log-likelihood (with epsilon added for numerical stability)\n",
    "    nll = -np.mean(y * np.log(y_pred + 1e-10) + (1 - y) * np.log(1 - y_pred + 1e-10))\n",
    "    \n",
    "    # Regularization term: apply only to non-intercept coefficients (beta[1:])\n",
    "    reg = (1 / (2 * tau**2)) * np.sum(beta[1:] ** 2)\n",
    "    \n",
    "    return nll + reg\n",
    "\n",
    "# Compute the gradient of the negative log-posterior\n",
    "def bayes_gradient(X, y, beta, tau):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the negative log-posterior for Bayesian Logistic Regression.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Feature matrix with intercept column.\n",
    "        y (ndarray): Binary target vector.\n",
    "        beta (ndarray): Coefficient vector (including intercept).\n",
    "        tau (float): Prior standard deviation.\n",
    "    \n",
    "    Returns:\n",
    "        gradient (ndarray): The gradient vector.\n",
    "    \"\"\"\n",
    "    z = np.dot(X, beta)\n",
    "    y_pred = sigmoid(z)\n",
    "    \n",
    "    # Gradient from the negative log-likelihood\n",
    "    grad = np.dot(X.T, (y_pred - y)) / len(y)\n",
    "    \n",
    "    # Add gradient from the regularization term (do not regularize intercept beta[0])\n",
    "    reg_grad = np.concatenate(([0], beta[1:])) / (tau**2)\n",
    "    \n",
    "    return grad + reg_grad\n",
    "\n",
    "# Gradient Descent for Bayesian Logistic Regression\n",
    "def train_bayesian_lr(X, y, tau, learning_rate=0.01, n_iterations=5000):\n",
    "    \"\"\"\n",
    "    Train Bayesian Logistic Regression using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Feature matrix with intercept column.\n",
    "        y (ndarray): Binary target vector.\n",
    "        tau (float): Prior standard deviation.\n",
    "        learning_rate (float): Learning rate for gradient descent.\n",
    "        n_iterations (int): Number of iterations.\n",
    "    \n",
    "    Returns:\n",
    "        beta (ndarray): Learned coefficient vector.\n",
    "    \"\"\"\n",
    "    beta = np.zeros(X.shape[1])  # Initialize coefficients\n",
    "    for i in range(n_iterations):\n",
    "        grad = bayes_gradient(X, y, beta, tau)\n",
    "        beta -= learning_rate * grad\n",
    "        if i % 100 == 0:\n",
    "            loss = bayes_loss(X, y, beta, tau)\n",
    "            print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "    return beta\n",
    "\n",
    "\n",
    "# Set hyperparameter for the prior (tau)\n",
    "tau = 1.0  # You can tune this value\n",
    "\n",
    "# Train the Bayesian Logistic Regression model\n",
    "beta_bayes = train_bayesian_lr(X_train, y_train, tau, learning_rate=0.01, n_iterations=5000)\n",
    "print(\"Estimated coefficients (Bayesian LR):\", beta_bayes)\n",
    "\n",
    "# Prediction function (same as before)\n",
    "def predict(X, beta):\n",
    "    z = np.dot(X, beta)\n",
    "    y_pred = sigmoid(z)\n",
    "    return (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate on the training set\n",
    "y_train_pred_bayes = predict(X_train, beta_bayes)\n",
    "train_accuracy_bayes = np.mean(y_train_pred_bayes == y_train)\n",
    "print(f\"\\nTraining Accuracy (Bayesian LR): {train_accuracy_bayes:.2%}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred_bayes = predict(X_test, beta_bayes)\n",
    "test_accuracy_bayes = np.mean(y_test_pred_bayes == y_test)\n",
    "print(f\"Test Accuracy (Bayesian LR): {test_accuracy_bayes:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy (Bayesian LR): 87.24%\n",
      "Test Accuracy (Bayesian LR): 87.32%\n",
      "\n",
      "Classification Report (Test Set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "Player B Win       0.87      0.87      0.87      6009\n",
      "Player A Win       0.87      0.87      0.87      6009\n",
      "\n",
      "    accuracy                           0.87     12018\n",
      "   macro avg       0.87      0.87      0.87     12018\n",
      "weighted avg       0.87      0.87      0.87     12018\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred_bayes)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred_bayes)\n",
    "\n",
    "print(f\"\\nTraining Accuracy (Bayesian LR): {train_accuracy:.2%}\")\n",
    "print(f\"Test Accuracy (Bayesian LR): {test_accuracy:.2%}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, y_test_pred_bayes, target_names=[\"Player B Win\", \"Player A Win\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
