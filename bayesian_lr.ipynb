{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Logistic Regression \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Head:\n",
      "  tourney_id tourney_name surface  draw_size tourney_level  tourney_date  \\\n",
      "0   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "1   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "2   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "3   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "4   2010-339     Brisbane    Hard         32             A      20100103   \n",
      "\n",
      "   match_num  winner_id     winner_name winner_hand  ...  l_1stIn l_1stWon  \\\n",
      "0          1     104053    Andy Roddick           R  ...     34.0     29.0   \n",
      "1         30     103285  Radek Stepanek           R  ...     27.0     14.0   \n",
      "2         29     104053    Andy Roddick           R  ...     43.0     34.0   \n",
      "3         28     103285  Radek Stepanek           R  ...     40.0     25.0   \n",
      "4         27     104792    Gael Monfils           R  ...     50.0     38.0   \n",
      "\n",
      "   l_2ndWon  l_SvGms l_bpSaved l_bpFaced  winner_rank winner_rank_points  \\\n",
      "0      11.0     10.0       3.0       5.0          7.0             4410.0   \n",
      "1       7.0      7.0       3.0       7.0         12.0             2625.0   \n",
      "2      21.0     13.0      10.0      12.0          7.0             4410.0   \n",
      "3      11.0     10.0       6.0      10.0         12.0             2625.0   \n",
      "4      17.0     14.0       3.0       6.0         13.0             2610.0   \n",
      "\n",
      "   loser_rank loser_rank_points  \n",
      "0        77.0             598.0  \n",
      "1        13.0            2610.0  \n",
      "2        20.0            1655.0  \n",
      "3       105.0             521.0  \n",
      "4        44.0             935.0  \n",
      "\n",
      "[5 rows x 45 columns]\n",
      "\n",
      "Columns in Dataset:\n",
      "Index(['tourney_id', 'tourney_name', 'surface', 'draw_size', 'tourney_level',\n",
      "       'tourney_date', 'match_num', 'winner_id', 'winner_name', 'winner_hand',\n",
      "       'winner_ht', 'winner_ioc', 'winner_age', 'loser_id', 'loser_name',\n",
      "       'loser_hand', 'loser_ht', 'loser_ioc', 'loser_age', 'score', 'best_of',\n",
      "       'round', 'minutes', 'w_ace', 'w_df', 'w_svpt', 'w_1stIn', 'w_1stWon',\n",
      "       'w_2ndWon', 'w_SvGms', 'w_bpSaved', 'w_bpFaced', 'l_ace', 'l_df',\n",
      "       'l_svpt', 'l_1stIn', 'l_1stWon', 'l_2ndWon', 'l_SvGms', 'l_bpSaved',\n",
      "       'l_bpFaced', 'winner_rank', 'winner_rank_points', 'loser_rank',\n",
      "       'loser_rank_points'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'feature_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 65\u001b[39m\n\u001b[32m     62\u001b[39m train_data = df_symmetric[df_symmetric[\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m] <= \u001b[32m2022\u001b[39m]\n\u001b[32m     63\u001b[39m test_data = df_symmetric[df_symmetric[\u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m] > \u001b[32m2022\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m X_train = train_data[\u001b[43mfeature_cols\u001b[49m].values\n\u001b[32m     66\u001b[39m y_train = train_data[\u001b[33m\"\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m\"\u001b[39m].values\n\u001b[32m     67\u001b[39m X_test = test_data[feature_cols].values\n",
      "\u001b[31mNameError\u001b[39m: name 'feature_cols' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"atp_matches_2010_2024_missing_handled.csv\")\n",
    "\n",
    "print(\"Dataset Head:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "print(\"\\nColumns in Dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Encode categorical variable: surface\n",
    "modelLR_df = pd.get_dummies(df, columns=[\"surface\"], drop_first=True)\n",
    "\n",
    "# Convert bool to int for one-hot encoded columns\n",
    "for col in [\"surface_Grass\", \"surface_Hard\"]:\n",
    "    modelLR_df[col] = modelLR_df[col].astype(int)\n",
    "\n",
    "modelLR_df[\"rank_diff\"] = modelLR_df[\"winner_rank\"] - modelLR_df[\"loser_rank\"]\n",
    "modelLR_df[\"ace_diff\"] = modelLR_df[\"w_ace\"] - modelLR_df[\"l_ace\"]\n",
    "modelLR_df[\"df_diff\"] = modelLR_df[\"w_df\"] - modelLR_df[\"l_df\"]\n",
    "modelLR_df[\"svpt_diff\"] = modelLR_df[\"w_svpt\"] - modelLR_df[\"l_svpt\"]\n",
    "modelLR_df[\"1stIn_diff\"] = modelLR_df[\"w_1stIn\"] - modelLR_df[\"l_1stIn\"]\n",
    "modelLR_df[\"1stWon_diff\"] = modelLR_df[\"w_1stWon\"] - modelLR_df[\"l_1stWon\"]\n",
    "modelLR_df[\"2ndWon_diff\"] = modelLR_df[\"w_2ndWon\"] - modelLR_df[\"l_2ndWon\"]\n",
    "modelLR_df[\"SvGms_diff\"] = modelLR_df[\"w_SvGms\"] - modelLR_df[\"l_SvGms\"]\n",
    "modelLR_df[\"bpSaved_diff\"] = modelLR_df[\"w_bpSaved\"] - modelLR_df[\"l_bpSaved\"]\n",
    "modelLR_df[\"bpFaced_diff\"] = modelLR_df[\"w_bpFaced\"] - modelLR_df[\"l_bpFaced\"]\n",
    "modelLR_df[\"age_diff\"] = modelLR_df[\"winner_age\"] - modelLR_df[\"loser_age\"]\n",
    "\n",
    "\n",
    "feature_cols = [\n",
    "    \"rank_diff\", \"ace_diff\", \"df_diff\", \"svpt_diff\", \"1stIn_diff\", \"1stWon_diff\",\n",
    "    \"2ndWon_diff\", \"SvGms_diff\", \"bpSaved_diff\", \"bpFaced_diff\", \"age_diff\",\n",
    "    \"surface_Grass\", \"surface_Hard\"  # Assuming Clay is the reference\n",
    "    ]\n",
    "\n",
    "# Target: 1 if winner is Player A (recorded winner), 0 if loser wins (flip for symmetry later)\n",
    "modelLR_df[\"target\"] = 1\n",
    "\n",
    "# Create a symmetric dataset by flipping winner/loser\n",
    "df_flipped = modelLR_df.copy()\n",
    "\n",
    "df_flipped[\"rank_diff\"] = -df_flipped[\"rank_diff\"]\n",
    "df_flipped[\"ace_diff\"] = -df_flipped[\"ace_diff\"]\n",
    "df_flipped[\"df_diff\"] = -df_flipped[\"df_diff\"]\n",
    "df_flipped[\"svpt_diff\"] = -df_flipped[\"svpt_diff\"]\n",
    "df_flipped[\"1stIn_diff\"] = -df_flipped[\"1stIn_diff\"]\n",
    "df_flipped[\"1stWon_diff\"] = -df_flipped[\"1stWon_diff\"]\n",
    "df_flipped[\"2ndWon_diff\"] = -df_flipped[\"2ndWon_diff\"]\n",
    "df_flipped[\"SvGms_diff\"] = -df_flipped[\"SvGms_diff\"]\n",
    "df_flipped[\"bpSaved_diff\"] = -df_flipped[\"bpSaved_diff\"]\n",
    "df_flipped[\"bpFaced_diff\"] = -df_flipped[\"bpFaced_diff\"]\n",
    "df_flipped[\"age_diff\"] = -df_flipped[\"age_diff\"]\n",
    "df_flipped[\"target\"] = 0\n",
    "\n",
    "# Combine original and flipped data\n",
    "df_symmetric = pd.concat([modelLR_df, df_flipped], ignore_index=True)\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "df_symmetric[\"tourney_date\"] = pd.to_datetime(df_symmetric[\"tourney_date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df_symmetric = df_symmetric.dropna(subset=[\"tourney_date\"])\n",
    "df_symmetric[\"year\"] = df_symmetric[\"tourney_date\"].dt.year\n",
    "train_data = df_symmetric[df_symmetric[\"year\"] <= 2022]\n",
    "test_data = df_symmetric[df_symmetric[\"year\"] > 2022]\n",
    "\n",
    "X_train = train_data[feature_cols].values\n",
    "y_train = train_data[\"target\"].values\n",
    "X_test = test_data[feature_cols].values\n",
    "y_test = test_data[\"target\"].values\n",
    "\n",
    "# Debug: Check array shape and type\n",
    "print(\"\\nX_train shape:\", X_train.shape)\n",
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "\n",
    "# Verifying the shapes of the feature matrix and target vector\n",
    "print(\"\\nFeature Matrix Shape:\", X.shape)\n",
    "print(\"Target Vector Shape:\", y.shape)\n",
    "\n",
    "# Step 3: Normalize features\n",
    "X_train_mean = np.nanmean(X_train, axis=0)\n",
    "X_train_std = np.nanstd(X_train, axis=0)\n",
    "X_train_std[X_train_std == 0] = 1e-10  # Avoid division by zero with small epsilon\n",
    "X_train = np.where(np.isnan(X_train), 0, X_train)\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_test = np.where(np.isnan(X_test), 0, X_test)\n",
    "X_test = (X_test - X_train_mean) / X_train_std\n",
    "\n",
    "# Add intercept term\n",
    "X_train = np.hstack([np.ones((X_train.shape[0], 1)), X_train])\n",
    "X_test = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the BLR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid function (with clipping to prevent numerical issues)\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "# Compute the negative log-posterior (loss) for Bayesian logistic regression.\n",
    "def bayes_loss(X, y, beta, tau):\n",
    "    \"\"\"\n",
    "    Computes the negative log-posterior (loss) for Bayesian Logistic Regression.\n",
    "    The loss is the negative log-likelihood plus the regularization term from the Gaussian prior.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Feature matrix with intercept column (shape: [n_samples, n_features+1])\n",
    "        y (ndarray): Binary target vector (shape: [n_samples])\n",
    "        beta (ndarray): Coefficient vector (including intercept) (shape: [n_features+1])\n",
    "        tau (float): Prior standard deviation.\n",
    "    \n",
    "    Returns:\n",
    "        loss (float): The computed loss.\n",
    "    \"\"\"\n",
    "    # Compute predictions\n",
    "    z = np.dot(X, beta)\n",
    "    y_pred = sigmoid(z)\n",
    "    \n",
    "    # Negative log-likelihood (with epsilon added for numerical stability)\n",
    "    nll = -np.mean(y * np.log(y_pred + 1e-10) + (1 - y) * np.log(1 - y_pred + 1e-10))\n",
    "    \n",
    "    # Regularization term: apply only to non-intercept coefficients (beta[1:])\n",
    "    reg = (1 / (2 * tau**2)) * np.sum(beta[1:] ** 2)\n",
    "    \n",
    "    return nll + reg\n",
    "\n",
    "# Compute the gradient of the negative log-posterior\n",
    "def bayes_gradient(X, y, beta, tau):\n",
    "    \"\"\"\n",
    "    Computes the gradient of the negative log-posterior for Bayesian Logistic Regression.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Feature matrix with intercept column.\n",
    "        y (ndarray): Binary target vector.\n",
    "        beta (ndarray): Coefficient vector (including intercept).\n",
    "        tau (float): Prior standard deviation.\n",
    "    \n",
    "    Returns:\n",
    "        gradient (ndarray): The gradient vector.\n",
    "    \"\"\"\n",
    "    z = np.dot(X, beta)\n",
    "    y_pred = sigmoid(z)\n",
    "    \n",
    "    # Gradient from the negative log-likelihood\n",
    "    grad = np.dot(X.T, (y_pred - y)) / len(y)\n",
    "    \n",
    "    # Add gradient from the regularization term (do not regularize intercept beta[0])\n",
    "    reg_grad = np.concatenate(([0], beta[1:])) / (tau**2)\n",
    "    \n",
    "    return grad + reg_grad\n",
    "\n",
    "# Gradient Descent for Bayesian Logistic Regression\n",
    "def train_bayesian_lr(X, y, tau, learning_rate=0.01, n_iterations=5000):\n",
    "    \"\"\"\n",
    "    Train Bayesian Logistic Regression using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "        X (ndarray): Feature matrix with intercept column.\n",
    "        y (ndarray): Binary target vector.\n",
    "        tau (float): Prior standard deviation.\n",
    "        learning_rate (float): Learning rate for gradient descent.\n",
    "        n_iterations (int): Number of iterations.\n",
    "    \n",
    "    Returns:\n",
    "        beta (ndarray): Learned coefficient vector.\n",
    "    \"\"\"\n",
    "    beta = np.zeros(X.shape[1])  # Initialize coefficients\n",
    "    for i in range(n_iterations):\n",
    "        grad = bayes_gradient(X, y, beta, tau)\n",
    "        beta -= learning_rate * grad\n",
    "        if i % 100 == 0:\n",
    "            loss = bayes_loss(X, y, beta, tau)\n",
    "            print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
    "    return beta\n",
    "\n",
    "\n",
    "# Set hyperparameter for the prior (tau)\n",
    "tau = 1.0  # You can tune this value\n",
    "\n",
    "# Train the Bayesian Logistic Regression model\n",
    "beta_bayes = train_bayesian_lr(X_train, y_train, tau, learning_rate=0.01, n_iterations=5000)\n",
    "print(\"Estimated coefficients (Bayesian LR):\", beta_bayes)\n",
    "\n",
    "# Prediction function (same as before)\n",
    "def predict(X, beta):\n",
    "    z = np.dot(X, beta)\n",
    "    y_pred = sigmoid(z)\n",
    "    return (y_pred >= 0.5).astype(int)\n",
    "\n",
    "# Evaluate on the training set\n",
    "y_train_pred_bayes = predict(X_train, beta_bayes)\n",
    "train_accuracy_bayes = np.mean(y_train_pred_bayes == y_train)\n",
    "print(f\"\\nTraining Accuracy (Bayesian LR): {train_accuracy_bayes:.2%}\")\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred_bayes = predict(X_test, beta_bayes)\n",
    "test_accuracy_bayes = np.mean(y_test_pred_bayes == y_test)\n",
    "print(f\"Test Accuracy (Bayesian LR): {test_accuracy_bayes:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
