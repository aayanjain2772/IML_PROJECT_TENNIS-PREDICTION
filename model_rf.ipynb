{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "\n",
    "# Simple Decision Tree using sklearn-like API\n",
    "class SimpleDecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "    \n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stop conditions\n",
    "        if (depth >= self.max_depth or n_samples < self.min_samples_split \n",
    "                or self.n_classes == 1):\n",
    "            self.leaf_value = mode(y).mode[0]\n",
    "            return\n",
    "        \n",
    "        # Select random subset of features\n",
    "        if self.max_features is not None:\n",
    "            feats_idx = np.random.choice(n_features, self.max_features, replace=False)\n",
    "        else:\n",
    "            feats_idx = np.arange(n_features)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feat, best_thresh, best_gain = None, None, -1\n",
    "        for feat in feats_idx:\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X[:, feat], threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_feat, best_thresh, best_gain = feat, threshold, gain\n",
    "        \n",
    "        if best_gain == -1:\n",
    "            self.leaf_value = mode(y).mode[0]\n",
    "            return\n",
    "        \n",
    "        # Store best split parameters\n",
    "        self.feat_idx = best_feat\n",
    "        self.threshold = best_thresh\n",
    "\n",
    "        # Split data\n",
    "        left_idxs = X[:, best_feat] <= best_thresh\n",
    "        right_idxs = X[:, best_feat] > best_thresh\n",
    "\n",
    "        # Recursive building\n",
    "        self.left = SimpleDecisionTree(\n",
    "            max_depth=self.max_depth, \n",
    "            min_samples_split=self.min_samples_split,\n",
    "            max_features=self.max_features\n",
    "        )\n",
    "        self.left.fit(X[left_idxs], y[left_idxs], depth+1)\n",
    "\n",
    "        self.right = SimpleDecisionTree(\n",
    "            max_depth=self.max_depth, \n",
    "            min_samples_split=self.min_samples_split,\n",
    "            max_features=self.max_features\n",
    "        )\n",
    "        self.right.fit(X[right_idxs], y[right_idxs], depth+1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs) for inputs in X])\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        if hasattr(self, 'leaf_value'):\n",
    "            return self.leaf_value\n",
    "        if inputs[self.feat_idx] <= self.threshold:\n",
    "            return self.left._predict(inputs)\n",
    "        else:\n",
    "            return self.right._predict(inputs)\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    def _information_gain(self, y, X_col, split_thresh):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs = X_col <= split_thresh\n",
    "        right_idxs = X_col > split_thresh\n",
    "        \n",
    "        if sum(left_idxs) == 0 or sum(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n = len(y)\n",
    "        n_left, n_right = sum(left_idxs), sum(right_idxs)\n",
    "\n",
    "        e_left, e_right = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right\n",
    "        \n",
    "        return parent_entropy - child_entropy\n",
    "\n",
    "# Random Forest classifier built from scratch\n",
    "class CustomRandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=10, min_samples_split=2, max_features='sqrt', random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if self.max_features == 'sqrt':\n",
    "            max_features = int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            max_features = int(np.log2(n_features))\n",
    "        else:\n",
    "            max_features = n_features\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample, y_sample = X[idxs], y[idxs]\n",
    "            tree = SimpleDecisionTree(\n",
    "                max_depth=self.max_depth, \n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_features=max_features\n",
    "            )\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return mode(tree_preds, axis=0).mode[0]\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == y)\n",
    "\n",
    "# Evaluation functions\n",
    "def custom_accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def custom_classification_report(y_true, y_pred, target_names):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    report = {}\n",
    "    for label, name in zip(unique_labels, target_names):\n",
    "        support = np.sum(y_true == label)\n",
    "        tp = np.sum((y_true == label) & (y_pred == label))\n",
    "        fp = np.sum((y_true != label) & (y_pred == label))\n",
    "        fn = np.sum((y_true == label) & (y_pred != label))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        report[name] = {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, \"support\": support}\n",
    "    overall_accuracy = custom_accuracy_score(y_true, y_pred)\n",
    "    return report, overall_accuracy\n",
    "\n",
    "# Data preprocessing\n",
    "modelLR_df = pd.read_csv(\"atp_matches_2010_2024_missing_handled.csv\")\n",
    "\n",
    "# Create engineered features\n",
    "modelLR_df[\"rank_diff\"] = modelLR_df[\"winner_rank\"] - modelLR_df[\"loser_rank\"]\n",
    "modelLR_df[\"ace_diff\"] = modelLR_df[\"w_ace\"] - modelLR_df[\"l_ace\"]\n",
    "modelLR_df[\"df_diff\"] = modelLR_df[\"w_df\"] - modelLR_df[\"l_df\"]\n",
    "modelLR_df[\"svpt_diff\"] = modelLR_df[\"w_svpt\"] - modelLR_df[\"l_svpt\"]\n",
    "modelLR_df[\"1stIn_diff\"] = modelLR_df[\"w_1stIn\"] - modelLR_df[\"l_1stIn\"]\n",
    "modelLR_df[\"1stWon_diff\"] = modelLR_df[\"w_1stWon\"] - modelLR_df[\"l_1stWon\"]\n",
    "modelLR_df[\"2ndWon_diff\"] = modelLR_df[\"w_2ndWon\"] - modelLR_df[\"l_2ndWon\"]\n",
    "modelLR_df[\"SvGms_diff\"] = modelLR_df[\"w_SvGms\"] - modelLR_df[\"l_SvGms\"]\n",
    "modelLR_df[\"bpSaved_diff\"] = modelLR_df[\"w_bpSaved\"] - modelLR_df[\"l_bpSaved\"]\n",
    "modelLR_df[\"bpFaced_diff\"] = modelLR_df[\"w_bpFaced\"] - modelLR_df[\"l_bpFaced\"]\n",
    "modelLR_df[\"age_diff\"] = modelLR_df[\"winner_age\"] - modelLR_df[\"loser_age\"]\n",
    "\n",
    "# One-hot encode the categorical variable: surface\n",
    "modelLR_df = pd.get_dummies(modelLR_df, columns=[\"surface\"], drop_first=True)\n",
    "\n",
    "# Create symmetric dataset\n",
    "modelLR_df[\"target\"] = 1  # Original records: target=1 (Player A wins)\n",
    "df_flipped = modelLR_df.copy()\n",
    "flip_cols = [\"rank_diff\", \"ace_diff\", \"df_diff\", \"svpt_diff\", \"1stIn_diff\",\n",
    "             \"1stWon_diff\", \"2ndWon_diff\", \"SvGms_diff\", \"bpSaved_diff\", \"bpFaced_diff\", \"age_diff\"]\n",
    "for col in flip_cols:\n",
    "    df_flipped[col] = -df_flipped[col]\n",
    "df_flipped[\"target\"] = 0  # Flipped records: target=0 (Player B wins)\n",
    "df_symmetric = pd.concat([modelLR_df, df_flipped], ignore_index=True)\n",
    "\n",
    "# Split into train and test\n",
    "df_symmetric[\"tourney_date\"] = pd.to_datetime(df_symmetric[\"tourney_date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df_symmetric = df_symmetric.dropna(subset=[\"tourney_date\"])\n",
    "df_symmetric[\"year\"] = df_symmetric[\"tourney_date\"].dt.year\n",
    "\n",
    "train_data = df_symmetric[df_symmetric[\"year\"] <= 2022]\n",
    "test_data = df_symmetric[df_symmetric[\"year\"] > 2022]\n",
    "\n",
    "# Feature selection\n",
    "feature_cols = [\n",
    "    \"ace_diff\", \"df_diff\", \"1stIn_diff\", \"SvGms_diff\", \"age_diff\",\n",
    "    \"1stWon_diff\", \"2ndWon_diff\", \"bpSaved_diff\", \"bpFaced_diff\", \"svpt_diff\",\n",
    "    \"rank_diff\", \"surface_Grass\", \"surface_Hard\"  # Assuming Clay is the reference\n",
    "]\n",
    "\n",
    "X_train = train_data[feature_cols].values\n",
    "y_train = train_data[\"target\"].values\n",
    "X_test = test_data[feature_cols].values\n",
    "y_test = test_data[\"target\"].values\n",
    "\n",
    "# Train model\n",
    "rf_model = CustomRandomForest(n_estimators=10, max_depth=10, min_samples_split=2,\n",
    "                              max_features='sqrt', random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_preds = rf_model.predict(X_train)\n",
    "test_preds = rf_model.predict(X_test)\n",
    "\n",
    "acc_train = custom_accuracy_score(y_train, train_preds)\n",
    "acc_test = custom_accuracy_score(y_test, test_preds)\n",
    "\n",
    "print(\"Training Accuracy: {:.2%}\".format(acc_train))\n",
    "print(\"Test Accuracy: {:.2%}\".format(acc_test))\n",
    "\n",
    "# Generate classification report\n",
    "target_names = [\"Player B Win\", \"Player A Win\"]\n",
    "report, overall_accuracy = custom_classification_report(y_test, test_preds, target_names)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "for label, metrics in report.items():\n",
    "    print(f\"{label}:\")\n",
    "    print(\"  Precision: {:.2f}\".format(metrics[\"precision\"]))\n",
    "    print(\"  Recall:    {:.2f}\".format(metrics[\"recall\"]))\n",
    "    print(\"  F1-Score:  {:.2f}\".format(metrics[\"f1-score\"]))\n",
    "    print(\"  Support:   {}\".format(metrics[\"support\"]))\n",
    "print(\"Overall Accuracy: {:.2%}\".format(overall_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
