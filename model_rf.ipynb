{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 210\u001b[39m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m    208\u001b[39m rf_model = CustomRandomForest(n_estimators=\u001b[32m10\u001b[39m, max_depth=\u001b[32m10\u001b[39m, min_samples_split=\u001b[32m2\u001b[39m,\n\u001b[32m    209\u001b[39m                               max_features=\u001b[33m'\u001b[39m\u001b[33msqrt\u001b[39m\u001b[33m'\u001b[39m, random_state=\u001b[32m42\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m \u001b[43mrf_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[32m    213\u001b[39m train_preds = rf_model.predict(X_train)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 128\u001b[39m, in \u001b[36mCustomRandomForest.fit\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    122\u001b[39m X_sample, y_sample = X[idxs], y[idxs]\n\u001b[32m    123\u001b[39m tree = SimpleDecisionTree(\n\u001b[32m    124\u001b[39m     max_depth=\u001b[38;5;28mself\u001b[39m.max_depth, \n\u001b[32m    125\u001b[39m     min_samples_split=\u001b[38;5;28mself\u001b[39m.min_samples_split,\n\u001b[32m    126\u001b[39m     max_features=max_features\n\u001b[32m    127\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    129\u001b[39m \u001b[38;5;28mself\u001b[39m.trees.append(tree)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mSimpleDecisionTree.fit\u001b[39m\u001b[34m(self, X, y, depth)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Recursive building\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mself\u001b[39m.left = SimpleDecisionTree(\n\u001b[32m     52\u001b[39m     max_depth=\u001b[38;5;28mself\u001b[39m.max_depth, \n\u001b[32m     53\u001b[39m     min_samples_split=\u001b[38;5;28mself\u001b[39m.min_samples_split,\n\u001b[32m     54\u001b[39m     max_features=\u001b[38;5;28mself\u001b[39m.max_features\n\u001b[32m     55\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_idxs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_idxs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.right = SimpleDecisionTree(\n\u001b[32m     59\u001b[39m     max_depth=\u001b[38;5;28mself\u001b[39m.max_depth, \n\u001b[32m     60\u001b[39m     min_samples_split=\u001b[38;5;28mself\u001b[39m.min_samples_split,\n\u001b[32m     61\u001b[39m     max_features=\u001b[38;5;28mself\u001b[39m.max_features\n\u001b[32m     62\u001b[39m )\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.right.fit(X[right_idxs], y[right_idxs], depth+\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mSimpleDecisionTree.fit\u001b[39m\u001b[34m(self, X, y, depth)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Recursive building\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mself\u001b[39m.left = SimpleDecisionTree(\n\u001b[32m     52\u001b[39m     max_depth=\u001b[38;5;28mself\u001b[39m.max_depth, \n\u001b[32m     53\u001b[39m     min_samples_split=\u001b[38;5;28mself\u001b[39m.min_samples_split,\n\u001b[32m     54\u001b[39m     max_features=\u001b[38;5;28mself\u001b[39m.max_features\n\u001b[32m     55\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_idxs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_idxs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.right = SimpleDecisionTree(\n\u001b[32m     59\u001b[39m     max_depth=\u001b[38;5;28mself\u001b[39m.max_depth, \n\u001b[32m     60\u001b[39m     min_samples_split=\u001b[38;5;28mself\u001b[39m.min_samples_split,\n\u001b[32m     61\u001b[39m     max_features=\u001b[38;5;28mself\u001b[39m.max_features\n\u001b[32m     62\u001b[39m )\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.right.fit(X[right_idxs], y[right_idxs], depth+\u001b[32m1\u001b[39m)\n",
      "    \u001b[31m[... skipping similar frames: SimpleDecisionTree.fit at line 56 (7 times)]\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36mSimpleDecisionTree.fit\u001b[39m\u001b[34m(self, X, y, depth)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Recursive building\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[38;5;28mself\u001b[39m.left = SimpleDecisionTree(\n\u001b[32m     52\u001b[39m     max_depth=\u001b[38;5;28mself\u001b[39m.max_depth, \n\u001b[32m     53\u001b[39m     min_samples_split=\u001b[38;5;28mself\u001b[39m.min_samples_split,\n\u001b[32m     54\u001b[39m     max_features=\u001b[38;5;28mself\u001b[39m.max_features\n\u001b[32m     55\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_idxs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43mleft_idxs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28mself\u001b[39m.right = SimpleDecisionTree(\n\u001b[32m     59\u001b[39m     max_depth=\u001b[38;5;28mself\u001b[39m.max_depth, \n\u001b[32m     60\u001b[39m     min_samples_split=\u001b[38;5;28mself\u001b[39m.min_samples_split,\n\u001b[32m     61\u001b[39m     max_features=\u001b[38;5;28mself\u001b[39m.max_features\n\u001b[32m     62\u001b[39m )\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.right.fit(X[right_idxs], y[right_idxs], depth+\u001b[32m1\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mSimpleDecisionTree.fit\u001b[39m\u001b[34m(self, X, y, depth)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Stop conditions\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (depth >= \u001b[38;5;28mself\u001b[39m.max_depth \u001b[38;5;129;01mor\u001b[39;00m n_samples < \u001b[38;5;28mself\u001b[39m.min_samples_split \n\u001b[32m     19\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_classes == \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     \u001b[38;5;28mself\u001b[39m.leaf_value = \u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Select random subset of features\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.stats import mode\n",
    "import pandas as pd\n",
    "\n",
    "# Simple Decision Tree using sklearn-like API\n",
    "class SimpleDecisionTree:\n",
    "    def __init__(self, max_depth=None, min_samples_split=2, max_features=None):\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "    \n",
    "    def fit(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.n_classes = len(np.unique(y))\n",
    "        \n",
    "        # Stop conditions: empty data, max depth, min samples, or single class\n",
    "        if n_samples == 0 or depth >= self.max_depth or n_samples < self.min_samples_split or self.n_classes == 1:\n",
    "            if len(y) > 0:\n",
    "                self.leaf_value = mode(y).mode[0] if mode(y).mode.size > 0 else np.argmax(np.bincount(y))\n",
    "            else:\n",
    "                self.leaf_value = 0  # Default value for empty data (adjust if needed)\n",
    "            return\n",
    "        \n",
    "        # Select random subset of features\n",
    "        if self.max_features is not None:\n",
    "            feats_idx = np.random.choice(n_features, self.max_features, replace=False)\n",
    "        else:\n",
    "            feats_idx = np.arange(n_features)\n",
    "        \n",
    "        # Find best split\n",
    "        best_feat, best_thresh, best_gain = None, None, -1\n",
    "        for feat in feats_idx:\n",
    "            thresholds = np.unique(X[:, feat])\n",
    "            for threshold in thresholds:\n",
    "                gain = self._information_gain(y, X[:, feat], threshold)\n",
    "                if gain > best_gain:\n",
    "                    best_feat, best_thresh, best_gain = feat, threshold, gain\n",
    "        \n",
    "        # If no valid split is found, make it a leaf\n",
    "        if best_gain == -1:\n",
    "            self.leaf_value = mode(y).mode[0] if mode(y).mode.size > 0 else np.argmax(np.bincount(y))\n",
    "            return\n",
    "        \n",
    "        # Store best split parameters\n",
    "        self.feat_idx = best_feat\n",
    "        self.threshold = best_thresh\n",
    "\n",
    "        # Split data\n",
    "        left_idxs = X[:, best_feat] <= best_thresh\n",
    "        right_idxs = X[:, best_feat] > best_thresh\n",
    "\n",
    "        # Only recurse if both sides have data\n",
    "        if sum(left_idxs) > 0 and sum(right_idxs) > 0:\n",
    "            self.left = SimpleDecisionTree(\n",
    "                max_depth=self.max_depth, \n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_features=self.max_features\n",
    "            )\n",
    "            self.left.fit(X[left_idxs], y[left_idxs], depth+1)\n",
    "\n",
    "            self.right = SimpleDecisionTree(\n",
    "                max_depth=self.max_depth, \n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_features=self.max_features\n",
    "            )\n",
    "            self.right.fit(X[right_idxs], y[right_idxs], depth+1)\n",
    "        else:\n",
    "            # If split results in empty side, make it a leaf\n",
    "            self.leaf_value = mode(y).mode[0] if mode(y).mode.size > 0 else np.argmax(np.bincount(y))\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.array([self._predict(inputs) for inputs in X])\n",
    "\n",
    "    def _predict(self, inputs):\n",
    "        if hasattr(self, 'leaf_value'):\n",
    "            return self.leaf_value\n",
    "        if inputs[self.feat_idx] <= self.threshold:\n",
    "            return self.left._predict(inputs)\n",
    "        else:\n",
    "            return self.right._predict(inputs)\n",
    "    \n",
    "    def _entropy(self, y):\n",
    "        counts = np.bincount(y)\n",
    "        probabilities = counts / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in probabilities if p > 0])\n",
    "\n",
    "    def _information_gain(self, y, X_col, split_thresh):\n",
    "        parent_entropy = self._entropy(y)\n",
    "\n",
    "        left_idxs = X_col <= split_thresh\n",
    "        right_idxs = X_col > split_thresh\n",
    "        \n",
    "        if sum(left_idxs) == 0 or sum(right_idxs) == 0:\n",
    "            return 0\n",
    "        \n",
    "        n = len(y)\n",
    "        n_left, n_right = sum(left_idxs), sum(right_idxs)\n",
    "\n",
    "        e_left, e_right = self._entropy(y[left_idxs]), self._entropy(y[right_idxs])\n",
    "        child_entropy = (n_left / n) * e_left + (n_right / n) * e_right\n",
    "        \n",
    "        return parent_entropy - child_entropy\n",
    "\n",
    "# Random Forest classifier built from scratch\n",
    "class CustomRandomForest:\n",
    "    def __init__(self, n_estimators=10, max_depth=10, min_samples_split=2, max_features='sqrt', random_state=None):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.max_features = max_features\n",
    "        self.trees = []\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        np.random.seed(self.random_state)\n",
    "        self.trees = []\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        if self.max_features == 'sqrt':\n",
    "            max_features = int(np.sqrt(n_features))\n",
    "        elif self.max_features == 'log2':\n",
    "            max_features = int(np.log2(n_features))\n",
    "        else:\n",
    "            max_features = n_features\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            idxs = np.random.choice(n_samples, n_samples, replace=True)\n",
    "            X_sample, y_sample = X[idxs], y[idxs]\n",
    "            tree = SimpleDecisionTree(\n",
    "                max_depth=self.max_depth, \n",
    "                min_samples_split=self.min_samples_split,\n",
    "                max_features=max_features\n",
    "            )\n",
    "            tree.fit(X_sample, y_sample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        tree_preds = np.array([tree.predict(X) for tree in self.trees])\n",
    "        return mode(tree_preds, axis=0).mode[0]\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return np.mean(preds == y)\n",
    "\n",
    "# Evaluation functions\n",
    "def custom_accuracy_score(y_true, y_pred):\n",
    "    return np.mean(y_true == y_pred)\n",
    "\n",
    "def custom_classification_report(y_true, y_pred, target_names):\n",
    "    unique_labels = np.unique(y_true)\n",
    "    report = {}\n",
    "    for label, name in zip(unique_labels, target_names):\n",
    "        support = np.sum(y_true == label)\n",
    "        tp = np.sum((y_true == label) & (y_pred == label))\n",
    "        fp = np.sum((y_true != label) & (y_pred == label))\n",
    "        fn = np.sum((y_true == label) & (y_pred != label))\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0\n",
    "        report[name] = {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, \"support\": support}\n",
    "    overall_accuracy = custom_accuracy_score(y_true, y_pred)\n",
    "    return report, overall_accuracy\n",
    "\n",
    "# Data preprocessing\n",
    "modelLR_df = pd.read_csv(\"atp_matches_2010_2024_missing_handled.csv\")\n",
    "\n",
    "# Create engineered features\n",
    "modelLR_df[\"rank_diff\"] = modelLR_df[\"winner_rank\"] - modelLR_df[\"loser_rank\"]\n",
    "modelLR_df[\"ace_diff\"] = modelLR_df[\"w_ace\"] - modelLR_df[\"l_ace\"]\n",
    "modelLR_df[\"df_diff\"] = modelLR_df[\"w_df\"] - modelLR_df[\"l_df\"]\n",
    "modelLR_df[\"svpt_diff\"] = modelLR_df[\"w_svpt\"] - modelLR_df[\"l_svpt\"]\n",
    "modelLR_df[\"1stIn_diff\"] = modelLR_df[\"w_1stIn\"] - modelLR_df[\"l_1stIn\"]\n",
    "modelLR_df[\"1stWon_diff\"] = modelLR_df[\"w_1stWon\"] - modelLR_df[\"l_1stWon\"]\n",
    "modelLR_df[\"2ndWon_diff\"] = modelLR_df[\"w_2ndWon\"] - modelLR_df[\"l_2ndWon\"]\n",
    "modelLR_df[\"SvGms_diff\"] = modelLR_df[\"w_SvGms\"] - modelLR_df[\"l_SvGms\"]\n",
    "modelLR_df[\"bpSaved_diff\"] = modelLR_df[\"w_bpSaved\"] - modelLR_df[\"l_bpSaved\"]\n",
    "modelLR_df[\"bpFaced_diff\"] = modelLR_df[\"w_bpFaced\"] - modelLR_df[\"l_bpFaced\"]\n",
    "modelLR_df[\"age_diff\"] = modelLR_df[\"winner_age\"] - modelLR_df[\"loser_age\"]\n",
    "\n",
    "# One-hot encode the categorical variable: surface\n",
    "modelLR_df = pd.get_dummies(modelLR_df, columns=[\"surface\"], drop_first=True)\n",
    "\n",
    "# Create symmetric dataset\n",
    "modelLR_df[\"target\"] = 1  # Original records: target=1 (Player A wins)\n",
    "df_flipped = modelLR_df.copy()\n",
    "flip_cols = [\"rank_diff\", \"ace_diff\", \"df_diff\", \"svpt_diff\", \"1stIn_diff\",\n",
    "             \"1stWon_diff\", \"2ndWon_diff\", \"SvGms_diff\", \"bpSaved_diff\", \"bpFaced_diff\", \"age_diff\"]\n",
    "for col in flip_cols:\n",
    "    df_flipped[col] = -df_flipped[col]\n",
    "df_flipped[\"target\"] = 0  # Flipped records: target=0 (Player B wins)\n",
    "df_symmetric = pd.concat([modelLR_df, df_flipped], ignore_index=True)\n",
    "\n",
    "# Split into train and test\n",
    "df_symmetric[\"tourney_date\"] = pd.to_datetime(df_symmetric[\"tourney_date\"], format=\"%Y%m%d\", errors=\"coerce\")\n",
    "df_symmetric = df_symmetric.dropna(subset=[\"tourney_date\"])\n",
    "df_symmetric[\"year\"] = df_symmetric[\"tourney_date\"].dt.year\n",
    "\n",
    "train_data = df_symmetric[df_symmetric[\"year\"] <= 2022]\n",
    "test_data = df_symmetric[df_symmetric[\"year\"] > 2022]\n",
    "\n",
    "# Feature selection\n",
    "feature_cols = [\n",
    "    \"ace_diff\", \"df_diff\", \"1stIn_diff\", \"SvGms_diff\", \"age_diff\",\n",
    "    \"1stWon_diff\", \"2ndWon_diff\", \"bpSaved_diff\", \"bpFaced_diff\", \"svpt_diff\",\n",
    "    \"rank_diff\", \"surface_Grass\", \"surface_Hard\"  # Assuming Clay is the reference\n",
    "]\n",
    "\n",
    "X_train = train_data[feature_cols].values\n",
    "y_train = train_data[\"target\"].values\n",
    "X_test = test_data[feature_cols].values\n",
    "y_test = test_data[\"target\"].values\n",
    "\n",
    "# Train model\n",
    "rf_model = CustomRandomForest(n_estimators=10, max_depth=10, min_samples_split=2,\n",
    "                              max_features='sqrt', random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "train_preds = rf_model.predict(X_train)\n",
    "test_preds = rf_model.predict(X_test)\n",
    "\n",
    "acc_train = custom_accuracy_score(y_train, train_preds)\n",
    "acc_test = custom_accuracy_score(y_test, test_preds)\n",
    "\n",
    "print(\"Training Accuracy: {:.2%}\".format(acc_train))\n",
    "print(\"Test Accuracy: {:.2%}\".format(acc_test))\n",
    "\n",
    "# Generate classification report\n",
    "target_names = [\"Player B Win\", \"Player A Win\"]\n",
    "report, overall_accuracy = custom_classification_report(y_test, test_preds, target_names)\n",
    "\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "for label, metrics in report.items():\n",
    "    print(f\"{label}:\")\n",
    "    print(\"  Precision: {:.2f}\".format(metrics[\"precision\"]))\n",
    "    print(\"  Recall:    {:.2f}\".format(metrics[\"recall\"]))\n",
    "    print(\"  F1-Score:  {:.2f}\".format(metrics[\"f1-score\"]))\n",
    "    print(\"  Support:   {}\".format(metrics[\"support\"]))\n",
    "print(\"Overall Accuracy: {:.2%}\".format(overall_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
